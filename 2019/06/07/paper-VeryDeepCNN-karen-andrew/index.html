<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          Very Deep Convolutional Networks For Large-Scale Image Recognition - kwongyangBiog
        
    </title>

    <link rel="canonical" href="http://kwongyang.com/2019/06/07/paper-VeryDeepCNN-karen-andrew/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('background.jpg')
            /*post*/
        
    }
    
    #signature{
        background-image: url('/img/signature/nameSign-white.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                            
                              <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                            
                        </div>
                        <h1>Very Deep Convolutional Networks For Large-Scale Image Recognition</h1>
                        <h2 class="subheading">Karen Simonyan* &amp; Andrew Zisserman*</h2>
                        <span class="meta">
                            Posted by Kwong on
                            2019-06-07
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">KwongyangBiog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>本篇文章是来自牛津大学工程科学系视觉几何组Karen Simonyan* &amp; Andrew Zisserman*所作的<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Very Deep Convolutional Networks For Large-Scale Image Recognition</a></p>
<h2><span id="abstract">Abstract</span></h2>
<p>在本文中，我们研究了卷积网络的深度对其在大规模图像识别设置中的精度的影响。我们的主要贡献是使用一个非常小(3×3)卷积滤波器的架构对增加深度的网络进行了全面的评估，这表明通过将深度推进到16-19个权重层，可以显著改进原有的art配置。这些发现是我们ImageNet Challenge 2014提交的基础，我们的团队分别在本地化和分类方面获得了第一名和第二名。我们还展示了我们的表示可以很好地推广到其他数据集，在这些数据集中，它们可以获得最先进的结果。我们公开了两个性能最好的ConvNet模型，以便进一步研究如何在计算机视觉中使用深度视觉表示</p>
<h2><span id="introduction">Introduction</span></h2>
<p>卷积网络(ConvNets)最近在大规模图像和视频识别方面取得了巨大的成功(Krizhevsky et al.， 2012;Zeiler &amp; Fergus, 2013;Sermanet et al.， 2014;Simonyan &amp; Zisserman, 2014)由于大型公共图像存储库(如ImageNet (Deng et al.， 2009))和高性能计算系统(如gpu或大型分布式集群)(Dean et al.， 2012)的出现，这一目标成为可能。特别是ImageNet大尺度视觉识别在深度视觉识别体系结构的发展中发挥了重要作用挑战(ILSVRC) (Russakovsky et al.， 2014)，它作为几代大规模图像分类系统的测试平台，从高维浅特征编码(Perronnin et al.， 2010) (ILSVRC-2011获奖者)to deep ConvNets (Krizhevsky et al.，</p>
<ol start="2012">
<li>(ILSVRC-2012冠军)</li>
</ol>
<p>随着ConvNets越来越成为计算机视觉领域的一种商品，人们对Krizhevsky等人(2012)的原始架构进行了许多改进，以达到更好的准确性。例如，最好的提交给ILSVRC-2013 (Zeiler &amp; Fergus, 2013;Sermanet et al.， 2014)利用更小的接受窗口大小和更小的第一个卷积层的步幅。另一个改进是在整个图像和多个尺度上密集地训练和测试网络(Sermanet et al.2014;霍华德,2014)。在本文中，我们讨论了ConvNet架构设计的另一个重要方面——深度。为此，我们对架构的其他参数进行了修正，通过增加卷积层来稳步增加网络的深度，这是可行的，因为在所有层中都使用了非常小的卷积滤波器(3×3)。</p>
<p>因此,我们想出更精确的事先架构,不仅实现了先进的准确性在ILSVRC分类和本地化的任务,但也适用于其他图像识别的数据集,使用时,他们甚至实现性能优良的一个相对简单的管道(如深线性SVM分类的特性没有微调)。为了便于进一步的研究，我们发布了两个性能最好的模型1。论文的其余部分组织如下。在第二节中，我们描述了我们的对流结构。图像分类训练与评价的具体内容见第3节，第4节对ILSVRC分类任务的配置进行了比较。第五节总结全文。为了完整起见，我们还在附录A中描述和评估了ILSVRC-2014对象本地化系统，并讨论了附录B中对其他数据集的非常深入的特性的一般化。<br>
最后，附录C包含了主要论文修改的列表。</p>
<h2><span id="convent-configurations">Convent Configurations</span></h2>
<p>为了在一个公平的环境下测量增加的对流深度所带来的改善，我们所有人对ConvNet层配置使用相同的原则设计，灵感来自Ciresan等人。(2011);Krizhevsky等(2012)。在本节中，我们首先描述了ConvNet配置的一般布局(第2.1节)，然后详细描述了评估中使用的特定配置(第2.2节)。然后讨论我们的设计选择，并与2.3节中的现有技术进行比较。</p>
<h3><span id="architecture">Architecture</span></h3>
<p>在训练过程中，我们的ConvNets的输入是一个固定大小的224×224 RGB图像。我们所做的唯一预处理是从每个像素中减去训练集上计算的RGB平均值。图像通过卷积(conv.)层的堆栈传递，在这里我们使用具有非常小的接受域的过滤器:3×3(这是捕捉左/右、上/下、中心概念的最小大小)。在其中一种配置中，我们还使用了1×1卷积滤波器，它可以看作是输入通道的线性变换(其次是非线性)。卷积步长固定为1像素;凹凸层输入的空间填充使得卷积后的空间分辨率保持不变，即3×3个凹凸层的填充为1像素。空间池由五个最大池层执行，它们遵循一些对流层(不是所有对流层都遵循最大池)。Max-pooling在一个2×2像素的窗口上执行，步长为2。</p>
<p>卷积层的堆栈(在不同的体系结构中具有不同的深度)后面是三个全连接(FC)层:前两个层各有4096个信道，第三个层执行1000路ILSVRC分类，因此包含1000个信道(每个类一个信道)。最后一层是软max层。在所有网络中，完全连接层的配置是相同的。</p>
<p>所有隐层均配备校正(ReLU (Krizhevsky et al.， 2012))非线性。我们注意到，我们的网络(除了一个)没有一个包含本地响应正常化(LRN)标准化(Krizhevsky et al.， 2012):如第4节所示，这种标准化并没有提高ILSVRC数据集的性能，而是导致内存消耗和计算时间的增加。适用时，LRN层的参数为(Krizhevsky et al.， 2012)。</p>
<h3><span id="configurations">Configurations</span></h3>
<p>本文评估的ConvNet配置如表1所示，每列一个。在下面我们将提到网队的名字(A-E)。所有的配置都遵循2.1节中给出的通用设计，只是在深度上有所不同:与网络A中的11个权重层不同(8个对流和3个FC层)到网络E中的19个权重层(16个对流和3个FC层)。对流层的宽度(通道的数量)相当小，从第一层的64个通道开始，然后在每个最大池层之后增加2倍，直到达到512个通道。</p>
<p>在表2中，我们报告了每种配置的参数数量。虽然深度较大，但我们的网中权重的数量并不大于较浅的网中权重的数量，而较浅的网具有较大的对流层宽度和接受域(144M权重In (Sermanet et al.， 2014))。</p>
<h3><span id="discussion">Discussion</span></h3>
<p>我们的ConvNet配置与ILSVRC-2012 (Krizhevsky et al.， 2012)和ILSVRC-2013比赛(Zeiler &amp; Fergus，2013;Sermanet et al.， 2014)。而不是在第一个conv中使用相对较大的接受域。层(e.g. 11×11 with stride 4 in (Krizhevsky et al.2012)，或7×7 with stride 2 in (Zeiler &amp; Fergus, 2012)，2013; Sermanet et al.， 2014))，我们在整个网络中使用非常小的3×3个接受域，在每个像素处与输入卷积(用stride 1)。很容易看出一叠两张3×3个对流层(中间无空间池)有效接受域为5×5;这些层具有7×7的有效接受域。那么，我们通过使用三个3×3对流层而不是一个7×7层的堆栈得到了什么呢?首先，我们加入了三个非线性校正层，而不是一个单一的校正层，这使得决策函数更有辨别力。其次，我们减少了参数的数量:假设一个三层的3×3卷积堆栈的输入和输出都有C通道，堆栈参数化为3 32C2 = 27C2权值;同时，单个7×7卷积层将需要72C2 = 49C2参数，即81%以上。这可以看作是对7×7对流滤波器进行正则化，迫使它们通过3×3滤波器进行分解(在两者之间注入非线性)。</p>
<p>引入1×1凹凸层(构型C，表1)是在不影响凹凸层接受域的前提下，增加决策函数非线性的一种方法。尽管在我们的例子中，1×1卷积本质上是在相同维度的空间上的线性投影(输入和输出通道的数量相同)。修正函数引入了额外的非线性。值得注意的是，Lin等人(2014)的“网络中的网络”架构最近使用了1×1对流层。</p>
<p>Ciresan等人(2011)曾经使用过小型卷积滤波器，但是他们的网明显没有我们的网深，而且他们没有在大型ILSVRC数据集上进行评估。Goodfellow等(2014)将deep ConvNets(11权层)应用于街道编号识别任务，并表明，深度的增加会带来更好的性能。GoogLeNet (Szegedy et al.， 2014)是ilsvvc -2014分类任务中表现最好的一个条目，它是独立于我们的工作开发的，但与我们相似的是，它基于非常深的ConvNets(22个权层)和小型卷积滤波器(除3×3外，还使用1×1和5×5卷积)。然而，它们的网络拓扑结构比我们的更为复杂，并且在第一层大大降低了特征图的空间分辨率，以减少计算量。如4.5节所示，我们的模型优于Szegedy等人的模型。(2014)在单网络分类精度方面。</p>
<h2><span id="classfication-framework">Classfication Framework</span></h2>
<p>在前一节中，我们介绍了网络配置的细节。在本节中，我们将详细描述分类卷积神经网络的训练和评估。</p>
<h3><span id="training">Training</span></h3>
<p>ConvNet的训练过程一般遵循Krizhevsky等人(2012)的方法(除了从多尺度训练图像中采样输入作物外，如后面所述)。即利用小批量梯度下降法对多项logistic回归目标进行优化，从而进行训练(基于反向传播(LeCun et al.， 1989))。批量大小设置为256，动量设置为0.9。训练通过体重衰减(L2惩罚乘数设置为5·10−4)和前两层完全连接的dropout正则化(dropout ratio设置为0.5)。学习速率最初设置为10 - 2，当验证集精度停止提高时，学习速率降低了10倍。总共降低学习速度3次，迭代370K(74个epoch)后停止学习。我们推测，尽管相对于(Krizhevsky et al.， 2012)，我们的网络参数更多，深度更大，但由于(a)深度越大，conv越小，隐含的正则化作用，网络收敛所需的时间越短。过滤器尺寸;(b)预先初始化某些层面。</p>
<p>网络权值的初始化很重要，因为糟糕的初始化会由于深度网中梯度的不稳定性而阻碍学习。为了避免这个问题，我们从训练配置A开始(表1)，浅到可以用随机初始化进行训练。然后，在训练更深层次的架构时，我们初始化了前四个卷积层和最后三个与net A层完全连接的层(中间层是随机初始化的)。我们没有降低预初始化层的学习率，允许它们在学习过程中发生变化。对于随机初始化(如适用)，我们从均值为0、方差为10 - 2的正态分布中抽取权重。偏差初始值为零。值得注意的是，在提交论文后，我们发现可以使用Glorot &amp; Bengio(2010)的随机初始化过程，在不进行预训练的情况下初始化权重。</p>
<p>为了得到固定大小的224×224 ConvNet输入图像，从重新标度的训练图像中随机裁剪它们(每次SGD迭代一个图像)。为了进一步扩大训练集，作物进行了随机水平翻转和随机rgb颜色偏移(Krizhevsky et al.， 2012)。训练图像的重新缩放如下所示。</p>
<p>训练图像大小。设S为等热带重标训练图像的最小边，从中裁剪ConvNet输入(我们也称S为训练标度)。当裁剪尺寸固定为224×224时，原则上S可以取不小于224的任何值:对于S = 224，裁剪将捕获整幅图像的统计信息，完全覆盖训练图像的最小边。对于S远远大于224，裁剪将对应于图像的一小部分，包括一个小对象或一个对象部分。</p>
<p>我们考虑了设置训练规模S的两种方法。第一种方法是固定S，它对应于单尺度训练(注意，样本作物中的图像内容仍然可以表示多尺度图像统计量)。在我们的实验中，我们评估了在两个固定尺度下训练的模型:S =256(已广泛应用于现有技术中(Krizhevsky et al.， 2012;Zeiler &amp; Fergus, 2013;Sermanet et al.， 2014))， S = 384。给定一个ConvNet配置，我们首先使用S = 256训练网络。为了加快S = 384网络的训练速度，我们用S = 256的权值对其进行初始化，并使用较小的初始学习率10−3。</p>
<p>设置S的第二种方法是多尺度训练，其中每个训练图像都是通过从一定范围内随机采样S来单独重新标定的[Smin, Smax] (我们使用Smin = 256 and)Smax = 512)。由于图像中的对象可以是不同大小的，因此在训练时考虑到这一点是有益的。这也可以看作是训练集通过缩放抖动来增强，其中只有一个模型被训练来识别大范围的物体。由于速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调来训练多尺度模型，并使用固定的S = 384进行预训练。</p>
<h3><span id="testing">Testing</span></h3>
<p>在测试时，给定一个经过训练的卷积神经网络和一个输入图像，对其进行如下分类。首先，它被等热带重新标度到一个预定义的最小图像边，记作Q(我们也称它为测试标度)。我们注意到Q并不一定等于训练量表S(正如我们将在第4节中展示的，对每个S使用几个Q值可以提高性能).然后，以类似于(Sermanet et al.， 2014)的方式将网络密集地应用于重新标度的测试图像上。即首先将全连通层转换为卷积层(第一层FC层为7×7个卷积层，后两层FC层为1×1个卷积层)。然后将得到的全卷积网络应用于整个(未裁剪的)图像。结果是一个类得分图，通道的数量等于类的数量，并且空间分辨率可变，取决于输入图像的大小。最后，为了得到图像的固定大小的类分数向量，对类分数图进行空间平均(和池)。我们还通过图像的水平翻转增加了测试集;对原始图像和翻转后图像的软极大类后边缘进行平均，得到最终的图像得分.</p>
<p>由于全卷积网络应用于整个图像，因此不需要在测试时对多个作物进行采样(Krizhevsky et al.， 2012)，效率较低，需要对每个作物进行网络重新计算。同时，使用大量的农作物，如Szegedy等人所做的(2014)可以提高精度，因为与全卷积网络相比，它可以对输入图像进行更精细的采样.同时,multi-crop评价是密集的补充评价由于不同的卷积边界条件:当应用事先作物,卷积功能地图用0填充,在密集的情况下评估填充为同一作物自然来自邻近的部分图像(由于玲珑和空间池),这大大增加了整体网络接受域,所以捕捉更多的上下文。虽然我们相信在实践中多个计算时间的增加作物并不证明准确性的潜在收益,我们也评估网络供参考使用50作物/规模(5×5正则网格与2次),总共150种作物/ 3尺度,这是与所使用的144种作物/ 4尺度Szegedy et al。(2014)。</p>
<h3><span id="implementation-details">Implementation Details</span></h3>
<p>实现来源于公开c++ Caffe 工具箱(Jia,2013)(扩展2013年12月),但包含许多重要的修改,允许我们进行培训和评价多个gpu安装在一个系统,以及训练和评估全尺寸(uncropped)图像多尺度(如上所述)。多GPU训练利用数据的并行性，将每批训练图像分割成多个GPU批次，在每个GPU上并行处理。计算GPU批处理梯度后，对梯度进行平均，得到整个批处理的梯度。梯度计算是同步的在GPU上训练的结果和在GPU上训练的结果是一样的。</p>
<p>虽然最近提出了加速ConvNet培训的更复杂的方法(Krizhevsky, 2014)，在不同的网络层采用模型和数据并行性，我们发现我们的概念上简单得多的方案已经在一个现成的4-GPU系统上提供了3.75倍的速度，相比使用单一GPU。在一个配备了四个NVIDIA Titan黑色gpu的系统上，根据架构的不同，训练一个网络需要2-3周。</p>
<h2><span id="classification-experiments">Classification Experiments</span></h2>
<p>数据集。在本节中，我们给出了用上述方法实现的图像分类结果<br>
ILSVRC-2012数据集上的ConvNet架构(用于ILSVRC 2012-2014挑战)。该数据集包含1000个类的图像，分为三组:训练(130万幅图像)、验证(50K幅图像)和测试(10万幅带有helout类标签的图像)。采用前1位和前5位误差两种方法对分类性能进行了评价。前者是一种多类分类错误，即分类不正确的图像所占比例;后者是在ILSVRC中使用的主要评价准则，计算为地面真值类别超出前5个预测类别的图像所占比例。对于大部分实验，我们使用验证集作为测试集，在测试集上也进行了一定的实验，并作为“VGG”team entry提交到ILSVRC-2014比赛的官方ILSVRC服务器上(Russakovsky et al.， 2014)。</p>
<h3><span id="single-scale-evaluation">Single Scale Evaluation</span></h3>
<p>我们从评估单个ConvNet模型在单个尺度上的性能开始，使用第2.2节中描述的层配置。测试图像大小设置如下:Q = S为固定值<br>
，对于抖动的S∈[Smin, Smax]， Q = 0.5(Smin + Smax)。结果如表3所示。</p>
<p>首先，我们注意到使用局部响应规范化(A- lrn网络)并不能在没有任何规范化层的情况下改进模型A。因此，我们没有在更深层次的体系结构中使用标准化(B-E)。</p>
<p>其次，我们观察到分类误差随着对流网深度的增加而减小,值得注意的是，尽管深度相同，配置C(包含3个1×1对流层)的性能比配置D(使用3×3 conv)差。网络中的层。这表明，虽然额外的非线性确实有所帮助(C比B好)，但使用非平凡接受域的conve . filters捕捉空间上下文也很重要(D比C好)。</p>
<p>最后，训练时尺度抖动(S∈[256;即使在测试时使用单一尺度，也比在具有固定最小边(S = 256或S = 384)的图像上进行训练效果要好得多。这证实了通过尺度抖动增强训练集确实有助于捕获多尺度图像统计量。</p>
<h3><span id="multi-scale-evaluation">Multi-Scale Evaluation</span></h3>
<p>在单个尺度上评估了ConvNet模型之后，我们现在评估了测试时尺度抖动的影响。它包括在测试图像的几个重新标度的版本上运行一个模型(对应于不同的Q值)，然后对得到的类后验值求平均值。考虑到训练规模与测试规模之间的较大差异导致性能下降，采用固定S训练的模型在三种测试图像大小上进行评估，接近训练图像大小:Q =<br>
{S - 32, S, S + 32}。同时，训练时的尺度抖动使得网络在测试时可以应用到更大范围的尺度，所以变量S∈[Smin]训练的模型;在更大范围的Q = {Smin, 0.5(Smin + Smax)， Smax}下计算Smax]。</p>
<h3><span id="multi-crop-evaluation">Multi-Crop Evaluation</span></h3>
<p>在表5中，我们将密集对流评价与多作物评价进行了比较(详见3.2节)。我们还通过平均这两种评估技术的softmax输出来评估它们的互补性。从图中可以看出，使用多种作物的效果略好于密集评价，这两种方法确实是互补的，因为它们的结合效果都优于各自。如上所述，我们假设这是由于卷积边界条件的不同处理。</p>
<h3><span id="convnet-fusion">ConvNet Fusion</span></h3>
<p>到目前为止，我们评估了单个ConvNet模型的性能。在这一部分的实验中，我们结合了几个模型的输出，平均它们的软最大类后验。由于模型的互补性，这提高了性能，并在2012年(Krizhevsky et al.， 2012)和2013年(Zeiler &amp; Fergus, 2013;Sermanet et al .,<br>
2014)。</p>
<p>结果如表6所示。到ILSVRC提交时，我们只训练了单尺度网络和多尺度模型D(通过只微调全连接层而不是所有层)。所得到的7个网络集成具有7.3%的ILSVRC测试误差。提交之后，我们只考虑了两个性能最好的多尺度模型(构型D和E)，结果表明，采用密实度评价的试验误差为7.0%，采用密实度与多作物联合评价的试验误差为6.8%。作为参考，我们最好的单模型误差达到7.1%(模型E，表5)。</p>
<h3><span id="comparison-with-the-state-of-the-art">Comparison With The State Of The Art</span></h3>
<p>最后，我们将结果与表7中的技术状态进行比较。在分类任务中<br>
ILSVRC-2014挑战赛(Russakovsky et al.， 2014)，我们的“VGG”团队获得第二名.使用7个模型的综合测试误差7.3%。提交后，我们将错误率降低到6.8%使用两种模型。</p>
<p>从表7可以看出，我们的very deep ConvNets明显优于上一代模型，在ILSVRC-2012和ILSVRC-2013两届比赛中均取得了最好的成绩。我们的结果与分类任务的赢家(GoogLeNet with)相比也是有竞争力的)，并显著优于ILSVRC-2013获奖作品Clarifai，后者在有外部训练数据的情况下获得了11.2%的成绩，在没有外部训练数据的情况下获得了11.7%的成绩。这是值得注意的，考虑到我们的最佳结果是通过组合两个模型实现的——明显少于大多数模型<br>
ILSVRC提交。就单网性能而言，我们的架构达到了最好的结果(7.0%的测试错误)，比单一GoogLeNet的性能高出0.9%。值得注意的是，我们并没有背离LeCun等人(1989)的经典ConvNet架构，而是通过大幅度增加深度对其进行了改进。</p>
<h2><span id="conclusion">Conclusion</span></h2>
<p>这项工作中，我们评估了非常深的卷积网络(多达19个权重层)用于大规模图像分类。结果表明，表示深度有利于分类精度，使用传统的ConvNet体系结构可以实现ImageNet challenge数据集的最优性能(LeCun et al.， 1989;(Krizhevsky et al.， 2012).在附录中，我们还展示了我们的模型能够很好地概括广泛的任务和数据集，匹配或优于围绕较不深的图像表示构建的更复杂的识别管道。我们的研究结果再次证实了深度在视觉表现中的重要性。</p>
<h2><span id="主要内容">主要内容</span></h2>
<p>本篇文章主要使用了3x3的卷积核来代替5x5/7x7的卷积核来减少参数数量，并研究了卷积神经网络中深度对模型的影响。</p>
<p>主要研究方法是：</p>
<ol>
<li>在原有的方法(Krizhevsky等人在2012年提出)的基础上，固定了架构中的其他参数，并通过添加卷积层稳定地增加网络深度构建模型。</li>
<li>首先进行单一尺寸的测试，分别验证了</li>
</ol>
<ul>
<li>使用局部相应标准化网络（A-LRN）的性能并没有比未用标准化层的高.</li>
<li>增加深度会降低错误率，并在深度为19层时达到饱和。</li>
<li>训练集通过变化尺寸来进行数据增强的确能获取更多尺寸的图片统计信息。</li>
</ul>
<ol start="3">
<li>然后进行了多尺寸测试，证明了在测试时图片尺寸波动会使性能更好。</li>
<li>与现有的最好模型(ILSVRC-2014冠军GoogleNet)相比较。</li>
</ol>
<h2><span id="疑问">疑问</span></h2>
<ol>
<li>权值衰减是什么？</li>
<li>模型融合是怎样做的？</li>
<li>多重裁切是什么？</li>
</ol>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2019/06/10/convolutionNN/" data-toggle="tooltip" data-placement="top" title="卷积神经网络">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2019/06/06/restricted-boltzmann-machines/" data-toggle="tooltip" data-placement="top" title="受限波尔兹曼机">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                    <div class="comment">
                        <div id="disqus_thread" class="disqus-thread"></div>
                    </div>
                
                <!--PC版
                <!--  畅言
    				<div id="SOHUCS" ></div>
    				<script charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/changyan.js" ></script>
    				<script type="text/javascript">
    				window.changyan.api.config({
    				appid: 'cytA3uKz2',
    				conf: 'prod_ec34338db0725ff75fc0186d53e47702'
    				});
				</script>
                -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Abstract</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Introduction</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Convent Configurations</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Architecture</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">Configurations</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">Discussion</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Classfication Framework</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Training</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Testing</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.3.</span> <span class="toc-nav-text">Implementation Details</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Classification Experiments</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">Single Scale Evaluation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">Multi-Scale Evaluation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.3.</span> <span class="toc-nav-text">Multi-Crop Evaluation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.4.</span> <span class="toc-nav-text">ConvNet Fusion</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.5.</span> <span class="toc-nav-text">Comparison With The State Of The Art</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Conclusion</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">主要内容</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">疑问</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                        
                          <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://godweiyang.com/" target="_blank">Wei yang</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "your-disqus-ID";
    var disqus_identifier = "http://kwongyang.com/2019/06/07/paper-VeryDeepCNN-karen-andrew/";
    var disqus_url = "http://kwongyang.com/2019/06/07/paper-VeryDeepCNN-karen-andrew/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<script type="text/javascript" src="source/js/zooming.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                    <li>
                        <a target="_blank" href="https://user.qzone.qq.com/760030764">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-qq fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                
                
                    <li>
                        <a target="_blank" href="http://weibo.com/5044608147">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/Kwongy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Kwong 2019 
                    <br>
                     To my beloved lover
                </p>
                <p class="copyright text-muted">
                    <!-- hitwebcounter Code START -->
                    <img src="http://hitwebcounter.com/counter/counter.php?page=7066902&style=0003&nbdigits=1&type=page&initCount=1156" title="Home Remedies For Wrinkles" Alt="Home Remedies For Wrinkles"   border="0" >
					visitors since 2018/04/20,
					<span class="post-count">178.1k words altogether</span>
				</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://kwongyang.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="http://kwongyang.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

</html>
