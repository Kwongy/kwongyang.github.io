<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          SegNet — A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation - kwongyangBiog
        
    </title>

    <link rel="canonical" href="http://kwongyang.com/2019/06/25/paper-SegNet-imageSegmentation/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('background.jpg')
            /*post*/
        
    }
    
    #signature{
        background-image: url('/img/signature/nameSign-white.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                            
                              <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                            
                              <a class="tag" href="/tags/#SemanticSegmentation" title="SemanticSegmentation">SemanticSegmentation</a>
                            
                        </div>
                        <h1>SegNet — A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</h1>
                        <h2 class="subheading">Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla</h2>
                        <span class="meta">
                            Posted by Kwong on
                            2019-06-25
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">KwongyangBiog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>本篇论文是由Cambridge大学Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Senior Member所作的<a href="https://arxiv.org/pdf/1511.00561.pdf" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p>
<h2><span id="abstract">Abstract</span></h2><p>提出了一种新颖实用的深度全卷积神经网络结构——SegNet。该核心可训练分割引擎由编码器网络、相应的解码器网络和像素级分类层组成。所述编码器网络的结构在拓扑上与VGG16网络[1]。解码器网络的作用是将编码器的低分辨率特征映射为全输入分辨率特征映射，进行像素级分类。SegNet lies的新颖之处在于解码器向上采样其低分辨率输入特征图的方式。具体地说，解码器使用在相应编码器的最大池化步骤中计算的池化索引来执行非线性上采样。这消除了学习向上采样的需要。上采样后的图像是稀疏的，然后与可训练滤波器进行卷积，生成密集的特征图。我们将我们提出的架构与广泛采用的FCN[2]以及著名的DeepLab-LargeFOV[3]进行了比较。DeconvNet[4]架构。这个比较揭示了在获得良好的分割性能时所涉及的内存和精度之间的权衡。</p>
<p>egNet主要是由场景理解应用程序驱动的。因此，它的设计在内存和推理过程中的计算时间方面都是高效的。它在可训练参数的数量上也明显小于其他竞争架构，并且可以使用随机梯度下降进行端到端训练。我们还在道路场景和SUN RGB-D室内场景分割任务上对SegNet等架构进行了受控基准测试。这些定量评估表明，与其他体系结构相比，SegNet具有良好的性能，推理时间有竞争力，并且在内存方面推理效率最高。我们还在<a href="http://mi.eng.cam.ac.uk/projects/segnet/上提供了SegNet的Caffe实现和web演示。" target="_blank" rel="noopener">http://mi.eng.cam.ac.uk/projects/segnet/上提供了SegNet的Caffe实现和web演示。</a></p>
<h2><span id="introduction">Introduction</span></h2><p>语义分割具有广泛的应用范围，从场景理解，推断对象之间的支持关系到自主驾驶.依靠低级别视觉线索的早期方法已经被流行的机器学习算法所取代.特别的，深度学习后来在手写数字识别、语音、整图分类以及图片中的检测上都取得了成功[VGG] [GoogLeNet].现在图像分割领域也对这个方法很感兴趣[crfasrnn] [parsent]等.然而，近来的很多方法的都尽力直接采用设计来图像分类的方法进行语义分割.结果虽然令人鼓舞，但是比较粗糙[deeplab].这主要是因为max-pooling和sub-sampling减少了特征图的分辨率.我们设计SegNet的动机就是来自于对于为了语义分割而从低分辨率的特征图到输入分辨率映射的需要.这种映射也必须产生一些特征用于精确地边界定位. </p>
<p>我们的架构，SegNet，设计的目的是作为一种高效的语义分割架构.它主要是由道路现场理解应用的动机，需要建模外观（道路，建筑物），形状（汽车，行人）的能力，并了解不同类别（如道路和侧面行走）之间的空间关系（上下文）.在典型的道路场景中，大多数像素属于大型类，如道路，建筑物，因此网络必须产生平滑的分段.引擎还必须具有根据其形状来描绘对象的能力，尽管它们的尺寸很小.因此，在提取的图像表示中保留边界信息是重要的.从计算的角度来看，在推理过程中，网络需要保证在内存和计算时间两方面都是高效的.进行端到端的训练为了使用诸如随机梯度下降（SGD）之类的有效的权重更新技术来联合优化网络中所有权重的能力是一个额外的好处，因为它更容易重复.SegNet的设计源于需要符合这些标准. </p>
<p>SegNet中的编码网络和VGG16的卷积层是拓扑上相同的.我们移除了全连接层，这样可以使SegNet比其他许多近来的结构[FCN] [DeconvNet] [ParseNet] [Decoupled]显著的小并且训练起来更容易.SegNet的关键部件是解码器网络，由一个对应于每个编码器的解码器层次组成.其中，解码器使用从相应的编码器接受的max-pooling indices来进行输入特征图的非线性upsampling.这个想法来自设计用于无监督功能学习的架构.在解码网络中重用max-pooling indics有多个实践好处：（１）它改进了边界划分（２）减少了实现端到端训练的参数数量（３）这种upsampling的形式可以仅需要少量的修改而合并到任何编码－解码形式的架构[FCN] [crfasrnn]. </p>
<p>这篇论文的一个主要贡献是，我们对Segnet解码技术和广泛使用的FCN的分析.这是为了传达在设计分割架构中的实际权衡.近来许多分割的深度架构使用相同的编码网络，例如VGG16，但是在解码网络的形式、训练和推理上是不同的.另一个常见的特点是，这些网络通常有亿级别的训练参数，从而导致端到端的训练很困难[DeconvNet].训练困难导致了多阶段的训练[FCN]，或者添加一个与训练的网络结构如FCN[crfasrnn]，或者用辅助支持，例如在推理阶段使用区域proposals[DeconvNet]，或者使用分类和分割网络的不相交训练[Decoupled]，或者用额外的数据进行与训练[Parsenet]或者全训练[crfasrnn].另外，性能提升后处理技术也受到欢迎.尽管这些因素都很好的提高了在voc上的性能，但是他们的定量结果难以解决实现良好性能所必需的关键设计因素.因此我们分析了被用在这些方法[FCN] [DeconvNet]中的解码过程，并揭示了他们的优点和缺陷. </p>
<p>我们评估了SegNet在两种场景分割任务中的性能，分别是CamVid道路场景分割和SUN RGB-D室内场景分割.VOC12在过去很多年都有分割的居基准挑战.但是，这个任务的大部分都有一个或两个由高度多样的背景包围的前景类.这隐含地有利于用于检测的技术，如最近关于解耦分类分割网络的工作所示[Decoupled]，其中分类网络可以用大量弱标签数据进行训练，并且独立分割网络性能得到改善.[deeplab]的方法还使用分类网络的特征图和独立的CRF后处理技术来执行分割.性能也可以通过额外的推理辅助来增强，例如区域proposals[DeconvNet] [Edge Boxes].因此，它与场景理解不同之处在于，其目的是利用对象的共同出现以及其他空间上下文来执行可靠的分割.为了证明SegNet的高效性，我们展示了一个实时的道路场景分割的在线demo，来分割11类的自主驾驶兴趣类(如图1所示).图1中展示了从Google中找的一些随机道路图片和SUNRGB-D中产生的一些随机室内测试场景图片的分割结果. </p>
<p>论文的剩余部分组织如下.在Section 2我们回顾了近期的相关文献.在Section 3我们描述了SegNet架构和对它的分析.在Section 4我们评估了SegNet在室外和室内数据集上的性能.接下来是Section 5关于我们的方法的一般性讨论，指出未来的工作.Section 6是结论.</p>
<h2><span id="literature-review">Literature Review</span></h2><p>语义分割是一个十分活跃的研究课题，其中很大一部分作用是因为作为挑战的数据集[PASCALVOC] [SUN RGB-D] [KITTI].在深度学习到来之前，性能最好的方法大部分依赖于手工设计的特征来独立地分类像素.通常，将一块区域送入一个分类器例如Random Forest或者Boosting来预测中心像素的类概率.基于外观的特征或者sfM（不知道是什么）已经被发明用来进行CamVid道路场景理解的测试.后通过使用成对或更高阶的CRF来平滑来自分类器的每像素噪声预测（通常称为一元项）来提高精确度.最近的方法旨在通过尝试预测块中所有像素的标签，而不仅仅是中心像素来产生高质量的一元项.这改进了随机森林一元项的结果，但是薄结构化类被分类不佳.CamVid测试中性能最好的技术通过将对象检测输出与CRF框架中的分类器预测相结合来解决标签频率之间的不平衡.所有这些技术的结果表明需要改进的分类的特征. </p>
<p>自从NYU数据集的发布以来，室内RGBD像素级语义分割也得到欢迎.该数据集显示了深度通道改善分割的有用性.他们的方法使用诸如RGB-SIFT，depth-SIFT和像素位置的特征作为神经网络分类器的输入来预测像素一元项.然后使用CRF来光滑这个有噪音的一元项.使用更丰富的特征集进行改进，包括LBP和区域分割，以获得更高的准确性，然后是CRF.还要一些其他的方法，所有这些方法的共同属性是使用手工设计的特征来分类RGB或RGBD图像. </p>
<p>深层卷积神经网络对物体分类的成功最近引导研究人员利用其特征学习能力进行结构化预测问题，如分割.还尝试将设计用于对象分类的网络应用于分割，特别是通过在块中复制最深层特征以匹配图像尺寸.然而，所得到的分类是块状的.另一种方法是使用循环神经网络[RNN]合并了几个低分辨率预测来创建输入图像分辨率预测.这些技术已经是手工设计特征的改进，但是它们划定边界的能力差. </p>
<p>更新的深度结构[FCN] [DeconvNet] [CRFASRNN] [Decoupled]特别设计用于分割，通过学习解码或将低分辨率图像表示映射到像素点预测，提升了最先进的技术水平.上边几个网络的编码网络是用来产生低分辨率便是，都是使用的VGG16分类网络结构(13个卷基层和3个全连接层).这些编码网络的权重在ImageNet上进行了特殊的预训练.解码器网络在这些架构之间不同，并且是负责为每个像素生成多维特征以进行分类的部分.<br>全卷积网络（FCN）架构中的每个解码器都学习对其输入特征图进行上采样，并将其与相应的编码器特征图组合，以产生到下一个解码器的输入。它是一种在编码器网络中具有大量可训练参数的架构（参数个数134M），但是非常小的解码器网络（参数个数0.5M）.该网络的整体大小使得难以在相关任务上端到端地进行训练.因此，作者使用了阶段性的训练过程.这里，解码器网络中的每个解码器逐渐添加到现有的训练好的网络中.网络生长直到没有观察到进一步的性能提高.这种增长在三个解码器之后停止，因此忽略高分辨率特征图肯定会导致边缘信息的丢失[DeconvNet].除了训练的相关问题之外，解码器中重用编码器特征图的需求使其在测试时间内内存集约.我们更深入地研究这个网络，因为它是其他最新架构的核心[CRFASRNN] [ParseNet]. </p>
<p>通过使用循环神经网络（RNN）附加到FCN[CRFASRNN]并对其在大的数据集上[VOC] [COCO]进行微调，FCN的预测性能进一步得到改善.在使用FCN的特征表征能力的同时，RNN层模仿CRF的尖锐边界划分能力.它们比FCN-8显示出显着的改进，但也表明当使用更多训练数据训练FCN-8时，这种差异减小.当与基于FCN-8的架构联合训练时，CRF-RNN的主要优点被揭示出来.联合训练有助于其他最近的结果.有趣的是，反卷积网络[DeconvNet]的性能明显优于FCN，但是以更复杂的训练和推理为代价.这提出了一个问题，即随着核心前馈分割引擎的改进，CRF-RNN的感知优势是否会减少.无论如何，CRF-RNN网络可以附加到任何深度分段架构，包括SegNet.<br>多尺度的深层架构也被广泛采用.它们有两种风格，（i）使用几个尺度的输入图像和相应的深度特征提取网络，以及（ii）组合来自单个深层结构的不同层的特征图[ParseNet].通常的想法是使用多尺度提取的特征来提供局部和全局上下文[zoom-out]，并且早期编码层的使用特征图保留更高频率的细节，从而导致更尖锐的类边界.其中一些架构由于参数大小而难以训练.因此，与数据增加一起使用多阶段训练过程.推论过程由于特征提取的多个卷积路径使用也是复杂度比较高的.其他在他们的多尺度网络上附加了一个CRF，并共同训练他们.然而，这些在测试时间不是前馈的，需要优化才能确定MAP标签. </p>
<h2><span id="architecture">Architecture</span></h2><p>编码器部分使用的是VGG16的前13层卷积网络，可以尝试使用Imagenet上的预训练.我们还可以丢弃完全连接的层，有利于在最深的编码器输出处保留较高分辨率的特征图.与其他最近的架构[FCN] [DeconvNet]相比，这也减少了SegNet编码器网络中的参数数量（从134M到14.7M）.如表6所示. </p>
<p>每个编码器层具有对应的解码器层，因此解码器网络具有13层.最终解码器输出被馈送到多级soft-max分类器以独立地为每个像素产生类概率.<br>每个编码器由卷积层、批归一化层、RELU组成，之后，执行具有2×2窗口和步幅2（非重叠窗口）的最大池化，输出结果相当于系数为2的下采样.最大池化用于实现输入图像中小空间位移的平移不变性，子采样导致特征图中每个像素的大输入图像上下文（空间窗口）.由于最大池化和子采样的叠加，导致边界细节损失增大，因此必须在编码特征图中在sub-sampling之前捕获和储存边界信息.为了高效，我们只储存了max-pooling indices，原则上，对于每个2×2池化窗口，这可以使用2位来完成，因此与浮动精度的记忆特征图相比，存储效率更高.正如我们在本文稍后展示的那样，这种较低的内存存储会导致精确度的轻微损失，但仍然适用于实际应用(?).<br>SegNet的解码技术如图3所示. </p>
<p>解码器网络中的解码器使用来自对应的编码器特征图的存储的最大池化索引来上采样至其输入特征图.此步骤产生稀疏特征图.然后将这些特征图与可训练的解码器滤波器组卷积以产生密集的特征图.然后是BN.注意，最后一个解码器产生一个多通道的特征图，而不是3通道的(RGB).然后输入给一个softmax分类器.这个soft-max独立地分类每个像素，soft-max分类器的输出是K通道图像的概率，其中K是类的数量.预测的分割对应于在每个像素处具有最大概率的类.<br>与SegNet相比，U-Net（提出用于医学影像社区）不重复使用池化指标，而是将整个特征图（以更多内存为代价）传输到相应的解码器，并将其连接上采样（通过反卷积）解码器特征图.在网络架构中，U-Net中没有conv5和max-pool 5.另一方面，SegNet使用来自VGG网络的所有预先训练的卷积层权重作为预训练权重.</p>
<h3><span id="decoder-variants">Decoder Variants</span></h3><p>许多分段架构[FCN] [deeplab] [DeconvNet]共享相同的编码器网络，它们只是以其解码器网络的形式而变化.其中我们选择比较SegNet解码技术与广泛使用的完全卷积网络（FCN）解码技术[FCN] [CRFASRNN].<br>为了分析SegNet并将其性能与FCN（解码器变体）进行比较，我们使用较小版本的SegNet，称为SegNet-Basic，它具有4个编码器和4个解码器.此外，选择所有编码器和解码器层的7×7的恒定核大小以提供用于平滑标记的宽上下文，即最深层特征图（层4）中的像素可以追溯到上下文窗口 106×106像素的输入图像.这种小尺寸的SegNet-Basic使我们能够在合理的时间内探索许多不同的变体（解码器）并进行训练.类似地，我们创建了FCN-Basic，一个可比较的FCN版本，用于我们的分析，它与SegNet-Basic共享相同的编码器网络，但与所有解码器中使用的FCN解码技术（见图3）相同.较小的变体是解码器滤波器是单通道的变体，即它们仅仅卷积它们相应的上采样特征图.该变体（SegNet-Basic-SingleChannelDecoder）显着减少了可训练参数的数量和推理时间(?).<br>FCN模型的重要设计元素是编码器特征图的降维步骤.这压缩了编码器特征图，然后在相应的解码器中使用.使用双线性插值权重初始化上采样内核.<br>我们还可以创建FCN-Basic模型的变体，该模型丢弃编码器特征映射添加步骤，并且仅学习上采样内核（FCN-Basic-NoAddition）.<br>除了上述变体之外，我们研究使用固定双线性插值权重的上采样，不需要上采样学习（双线性插值）.另一方面，我们可以在SeqNet解码器的每一层添加64个编码器特征映射到SegNet解码器的相应输出特征图，以创建更多内存扩大型SegNet（SegNet-Basic-EncoderAddition）.这里使用上采样的max-pooling indices，随后进行卷积步骤以使其稀疏输入变得更加密集.然后将其逐个添加到相应的编码器特征图，以产生解码器输出.<br>另一种和更多的内存密集型FCN-Basic变体（FCN-Basic-NoDimReduction）是对编码器特征映射没有进行维度降低的地方.这意味着与FCN-Basic不同，最终的编码器特征图在将其传送到解码器网络之前不会压缩到K个通道.因此，每个解码器结尾处的信道数量与相应的编码器相同(即64).<br>我们还尝试了其他通用变体，其中功能图只是通过复制进行上采样，或者通过使用固定（和稀疏）索引数组进行上采样.与上述变体相比，这些表现相当差.在编码器网络（解码器是冗余的）中没有最大池和子采样的变体消耗更多的存储器，需要更长的时间来收敛和执行等不好.最后，请注意，为了鼓励复制我们的结果，我们发布了Caffe执行所有变体.</p>
<h3><span id="training">Training</span></h3><p>我们使用CamVid路景数据集来对基于解码器变体的性能进行基准测试.该数据集很小，由360×480分辨率的367次训练和233次测试RGB图像（白天和黄昏场景）组成.挑战是划分道路，建筑，汽车，行人，标志，极点，侧路等11类.我们对RGB输入进行局部对比度归一化.<br>编码器和解码器权重都使用He等人的方法.为了训练所有的变体，我们使用固定学习率0.1和动量0.9的随机梯度下降（SGD），使用我们的Caffe实现SegNet-Basic.在每轮之前，训练集被洗牌，然后按顺序挑选每个小批量（12张图像），从而确保每个图像在一个时代只被使用一次.我们选择在验证数据集上执行最高的模型.<br>我们使用交叉熵损失作为训练网络的目标函数.损失在一个小批量的所有像素上求和得到.当训练集中的每个类别（例如，道路，天空和建筑像素占主导地位的CamVid数据集）中像素数量的变化很大时，则需要根据真实类别不同地加权.这被称为class<br>balancing.我们使用median frequency balancing，其中分配给损失函数中的类的权重是在整个训练集上计算的类频率的中值除以类频率的比率(?).这意味着训练集中的较大类的权重小于1，最小类的权重最高.我们还尝试了不同类型的训练，无需类平衡，也可以等效地使用natural frequency balancing.</p>
<h3><span id="analysis">Analysis</span></h3><p>为了定量分析不同的解码器变体.使用如下的测量:G值是global accuracy，测量数据集中所有像素正确分类的百分比.C值class average accuracy，所有类的预测准确度的平均值.还有就是在Pascal VOC12挑战中使用的所有类的mIoU.mIoU度量是比类平均精度更严格的度量，因为它惩罚了假阳性预测.然而，mIoU度量不是通过类平衡交叉熵损失直接优化的.<br>mIoU指标也被称为“雅克指数”，最常用于基准测试.然而，Csurka等人注意到，这个度量并不总是符合人类对质量好的细分的定性判断（等级）.他们以示例的形式表明，mIoU有利于区域平滑度，并且不评估边界准确性，FCN作者最近也提到了这一点.因此，他们建议通过基于通常用于评估无监督图像分割质量的伯克利轮廓匹配得分的边界测量来补充mIoU度量.Csurka等人简单地将其扩展到语义分割，并且表明与mIoU度量结合使用的语义轮廓精度的度量与分割输出的人类排序一致.<br>计算语义轮廓得分的关键思想是评估F1测量，涉及在给定一个像素公差距离的情况下计算预测和ground truth类边界的精确度和回调值.我们使用图像对角线的0.75％的值作为公差距离.将存在于地面真实测试图像中的每个类的F1测量值进行平均以产生图像F1度量.BF作为整个测试集的F1度量.<br>虽然我们在训练变体时使用类平衡，但仍然重要的是要实现高全局准确度，从而实现整体平滑分割.我们还观察到，当等级平均值最高时报告数值性能通常可以对应于表示感知噪声分割输出的低全局精度.<br>表1展示了我们的分析结果. </p>
<p>在最好的情况下，当内存和推理时间都不受约束时，诸如FCN-Basic-NoDimReduction和SegNet-EncoderAddition之类的较大型号比其他变体更准确.特别地，在FCN-Basic模型中丢弃维数降低导致具有高BF分数的FCN Basic变体中的最佳性能.这再次强调了分割架构中存储器与精度之间的权衡.<br>我们现在可以总结上述分析，具有以下一般要点:<br>1）编码器特征图全部存储时，性能最好。 这最明显地反映在语义轮廓描绘度量（BF）中.<br>2）当限制推理中的存储器时，可以使用适当的解码器（例如SegNet类型）来存储和使用编码器特征图（维数降低，最大聚集索引）的压缩形式来提高性能.<br>3）更大的解码器提高了给定编码器网络的性能.</p>
<h2><span id="benchmarking">Benchmarking</span></h2><p>我们拿SegNet和FCN、DeepLab-LargeFOV和DeconvNet进行了比较.定性结果如图4所示. </p>
<p>定性结果显示了所提出的架构在道路场景中分割较小类的能力，同时产生了整体场景的平滑分割.事实上，在受控的基准设置下，与一些较大的型号相比，SegNet显示出优异的性能.DeepLab-LargeFOV是最有效的模式，CRF后处理可以产生有竞争力的结果，尽管较小的类丢失.具有学习去卷积的FCN明显优于固定双线性上采样.DeconvNet是最大的模式，最无效的训练.它的预测不能保留小的类别. DeconvNet具有更高的边界划分精度，但与DeconvNet相比，SegNet效率更高.这可以从表中的计算统计数据中看出具有完全连接的层（变成卷积层）的FCN，DeconvNet以较慢的速度进行训练，并参考SegNet具有相当或更高的前后传递时间.在这里，我们还注意到，过拟合并不是训练这些更大型号的一个问题，因为在与SegNet进行的可比较的迭代中，他们的指标呈现增长趋势. </p>
<p>对于FCN模型，学习反卷积层，而不是用双线性插值法固定它们，提高了性能，特别是BF分数.它也在更短的时间内实现了更高的度量.<br>令人惊讶的是，DeepLab-LargeFOV是以45×60分辨率来预测标签的，却产生了很有竞争力的结果，因为它是参数化方面最小的模型，而且具有最快的训练时间，如表6所示.然而边界精度是 较差的，这和其他架构一样.DeconvNet的BF得分高于其他网络，训练了很长时间.<br>CRF导致G值和mIoU提高，但是C值降低，BF值也有很大提高.</p>
<h2><span id="discussion-and-furture-work">Discussion and furture work</span></h2><p>由于大量数据集的可用性和扩展的模型深度和参数化，深度学习模型往往取得了更大的成功.然而，在实践中，训练和测试期间的记忆和计算时间等因素是从大型模型库中选择模型时考虑的重要因素.训练时间成为一个重要的考虑因素，特别是当我们的实验显示，性能增益与增加的训练时间不相称时.测试时间记忆和计算负荷对于在专用嵌入式设备上部署模型（例如AR应用程序）很重要.从总体效率的角度来看，我们对于更小更多的内存，对于实时应用的时间效率模型（如道路现场理解和AR）的关注较少.这是SegNet提案的主要动机，它比其他竞争的架构明显更小，更快，但是我们已经表现出对于道路现场理解等任务的效率.<br>诸如Pascal和MS-COCO之类的分割挑战是对象分割挑战，其中几个类别存在于任何测试图像中.场景分割更具挑战性，因为室内场景的高度变化，同时需要分割更多的类.户外和室内场景分割的任务在现代应用中也更为实用，如自主驾驶，机器人和AR.<br>我们选择了对各种深层分割架构（如边界F1测量（BF））进行基准测量的指标，以补充更偏向于区域精度的现有指标.从我们的实验和其他独立的基准可以看出，从移动的汽车捕获的室外场景图像更容易分割，深层结构能够很好地运行.我们希望我们的实验将鼓励研究人员注意更具挑战性的室内场景分割任务.<br>在对不同参数化的不同深层架构进行基准测试时，我们必须做出的一个重要选择是训练他们的方式.许多这些架构已经使用了许多支持技术和多阶段训练配方来达到数据集的高准确度，但是这使得很难在时间和内存限制下收集关于其真实性能的证据.相反，我们选择执行受控的基准测试，我们使用批处理标准化，使用相同的求解器（SGD）实现端对端训练.然而，我们注意到，这种方法不能完全解开模型与求解器（优化）在实现特定结果时的影响.这主要是由于训练这些网络涉及梯度反向传播，这是不完美的，优化是非常大的非凸的问题.承认这些缺点，我们希望这种受控分析补充了其他基准，并揭示了涉及不同知名架构的实际权衡.<br>对于未来，我们希望利用我们对从分析中收集到的分段架构的理解，为实时应用设计更有效的架构.我们也有兴趣从深度分段架构中估计预测的模型不确定性.</p>
<h2><span id="conclusion">Conclusion</span></h2><p>我们提出了SegNet，一种用于语义分割的深度卷积网络架构.SegNet背后的主要动机是需要设计一种有效的道路和室内场景理解架构，这在存储和计算时间方面都是有效的.我们分析了SegNet，并将其与其他重要变体进行了比较，以揭示涉及设计分段架构的实际权衡，特别是训练时间，内存与精度.存储编码器网络特征的那些架构完整性能最好，但在推理时间消耗更多的内存.另一方面，SegNet更有效率，因为它仅存储特征映射的最大池索引，并将其用于解码器网络以实现良好的性能.在大型和众所周知的数据集中，SegNet具有竞争力，实现道路现场理解的高分.深层分割架构的端到端学习是一个更难的挑战，我们希望更多地关注这一重要问题.</p>
<h2><span id="收获">收获</span></h2><p>SegNet使用Encoder-Decoder结构</p>
<p>SegNet的编码器部分使用的是VGG16的前13层卷积网络，每个编码器层都对应一个解码器层，最终解码器的输出被送入soft-max分类器以独立的为每个像素产生类概率。<br>SegNet中的Pooling与其他Pooling多了一个index功能（该文章亮点之一），也就是每次Pooling，都会保存通过max选出的权值在2x2 filter中的相对位置。SegNet在Unpooling时用index信息，直接将数据放回对应位置，后面再接Conv训练学习。这个上采样不需要训练学习</p>
<p>突出贡献:<br>分割的精度略好于FCN,总体效率也比FCN略高,</p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2019/06/27/paper-DeepLabV1/" data-toggle="tooltip" data-placement="top" title="DeepLab-Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2019/06/18/paper-fullyConvolutionalNetwork/" data-toggle="tooltip" data-placement="top" title="Fully Convolutional Networks for Semantic Segmentation">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                    <div class="comment">
                        <div id="disqus_thread" class="disqus-thread"></div>
                    </div>
                
                <!--PC版
                <!--  畅言
    				<div id="SOHUCS" ></div>
    				<script charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/changyan.js" ></script>
    				<script type="text/javascript">
    				window.changyan.api.config({
    				appid: 'cytA3uKz2',
    				conf: 'prod_ec34338db0725ff75fc0186d53e47702'
    				});
				</script>
                -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Abstract</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Introduction</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Literature Review</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Architecture</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Decoder Variants</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Training</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">4.3.</span> <span class="toc-nav-text">Analysis</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Benchmarking</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Discussion and furture work</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">Conclusion</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#null"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">收获</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                        
                          <a class="tag" href="/tags/#Paper" title="Paper">Paper</a>
                        
                          <a class="tag" href="/tags/#SemanticSegmentation" title="SemanticSegmentation">SemanticSegmentation</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://godweiyang.com/" target="_blank">Wei yang</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "your-disqus-ID";
    var disqus_identifier = "http://kwongyang.com/2019/06/25/paper-SegNet-imageSegmentation/";
    var disqus_url = "http://kwongyang.com/2019/06/25/paper-SegNet-imageSegmentation/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<script type="text/javascript" src="source/js/zooming.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                    <li>
                        <a target="_blank" href="https://user.qzone.qq.com/760030764">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-qq fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                
                
                    <li>
                        <a target="_blank" href="http://weibo.com/5044608147">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/Kwongy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Kwong 2019 
                    <br>
                     To my beloved lover
                </p>
                <p class="copyright text-muted">
                    <!-- hitwebcounter Code START -->
                    <img src="http://hitwebcounter.com/counter/counter.php?page=7066902&style=0003&nbdigits=1&type=page&initCount=1156" title="Home Remedies For Wrinkles" Alt="Home Remedies For Wrinkles"   border="0" >
					visitors since 2018/04/20,
					<span class="post-count">173.6k words altogether</span>
				</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://kwongyang.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="http://kwongyang.com/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

</html>
